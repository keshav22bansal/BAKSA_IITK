{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_Sentiment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdFQndZfC8XO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1xQ8j3PpN5m",
        "colab_type": "code",
        "outputId": "e4f6bb30-6602-404d-f421-b207d6ca4407",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')  "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQQ6FkNcv-ae",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_path = \"/content/gdrive/My Drive/nlp_project/\"\n",
        "train_name = \"spanglish_train_demojised.txt\"\n",
        "validation_name = \"spanglish_validation_demojised.txt\"\n",
        "model_save_name = \"spanglish_model.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThOo8L-9sU7n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append(data_path)\n",
        "sys.path.append(data_path)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hebOrRmGWfj",
        "colab_type": "code",
        "outputId": "3dde9c28-cd95-4b97-b745-0feb604f2cdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "!pip install transformers"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.3.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.47)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.47)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->transformers) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ao9CbNtzv5lF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzN9lV1ZGhY2",
        "colab_type": "code",
        "outputId": "79fa657e-d204-43b9-aebc-dc68f2123eab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YriUeQu3G47w",
        "colab_type": "code",
        "outputId": "ec0af30c-6cbe-4a45-81e3-5c91de5f0547",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "len(tokenizer.vocab)\n",
        "tokens = tokenizer.tokenize('how ARE you ')\n",
        "\n",
        "print(tokens)\n",
        "indexes = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(indexes)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['how', 'AR', '##E', 'you']\n",
            "[1293, 22133, 2036, 1128]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_lDr2XzHEuN",
        "colab_type": "code",
        "outputId": "efeeeb61-6db8-431c-8815-0efada32faa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "init_token = tokenizer.cls_token\n",
        "eos_token = tokenizer.sep_token\n",
        "pad_token = tokenizer.pad_token\n",
        "unk_token = tokenizer.unk_token\n",
        "\n",
        "print(init_token, eos_token, pad_token, unk_token)\n",
        "\n",
        "\n",
        "# We can get the indexes of the special tokens by converting them using the vocabulary...\n",
        "\n",
        "# In[7]:\n",
        "\n",
        "\n",
        "init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
        "eos_token_idx = tokenizer.convert_tokens_to_ids(eos_token)\n",
        "pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
        "unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
        "\n",
        "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)\n",
        "\n",
        "\n",
        "# ...or by explicitly getting them from the tokenizer.\n",
        "\n",
        "# In[8]:\n",
        "\n",
        "\n",
        "init_token_idx = tokenizer.cls_token_id\n",
        "eos_token_idx = tokenizer.sep_token_id\n",
        "pad_token_idx = tokenizer.pad_token_id\n",
        "unk_token_idx = tokenizer.unk_token_id\n",
        "\n",
        "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)\n",
        "\n",
        "\n",
        "# Another thing we need to handle is that the model was trained on sequences with a defined maximum length - it does not know how to handle sequences longer than it has been trained on. We can get the maximum length of these input sizes by checking the `max_model_input_sizes` for the version of the transformer we want to use. In this case, it is 512 tokens.\n",
        "\n",
        "# In[9]:\n",
        "\n",
        "\n",
        "max_input_length = tokenizer.max_model_input_sizes['bert-base-cased']\n",
        "\n",
        "print(max_input_length)\n",
        "\n",
        "\n",
        "# Previously we have used the `spaCy` tokenizer to tokenize our examples. However we now need to define a function that we will pass to our `TEXT` field that will handle all the tokenization for us. It will also cut down the number of tokens to a maximum length. Note that our maximum length is 2 less than the actual maximum length. This is because we need to append two tokens to each sequence, one to the start and one to the end.\n",
        "\n",
        "# In[10]:\n",
        "\n",
        "\n",
        "def tokenize_and_cut(sentence):\n",
        "    tokens = tokenizer.tokenize(sentence) \n",
        "    tokens = tokens[:max_input_length-2]\n",
        "    return tokens\n",
        "\n",
        "\n",
        "# Now we define our fields. The transformer expects the batch dimension to be first, so we set `batch_first = True`. As we already have the vocabulary for our text, provided by the transformer we set `use_vocab = False` to tell torchtext that we'll be handling the vocabulary side of things. We pass our `tokenize_and_cut` function as the tokenizer. The `preprocessing` argument is a function that takes in the example after it has been tokenized, this is where we will convert the tokens to their indexes. Finally, we define the special tokens - making note that we are defining them to be their index value and not their string value, i.e. `100` instead of `[UNK]` This is because the sequences will already be converted into indexes.\n",
        "# \n",
        "# We define the label field as before.\n",
        "\n",
        "# In[87]:\n",
        "\n",
        "\n",
        "from torchtext import data\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[CLS] [SEP] [PAD] [UNK]\n",
            "101 102 0 100\n",
            "101 102 0 100\n",
            "512\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unFFscF5GkpA",
        "colab_type": "code",
        "outputId": "98b9791c-46de-4afb-cd33-7313fa3c3dd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "TEXT = data.Field(batch_first = True,\n",
        "                  use_vocab = False,\n",
        "                  tokenize = tokenize_and_cut,\n",
        "                  preprocessing = tokenizer.convert_tokens_to_ids,\n",
        "                  init_token = init_token_idx,\n",
        "                  eos_token = eos_token_idx,\n",
        "                  pad_token = pad_token_idx,\n",
        "                  unk_token = unk_token_idx)\n",
        "\n",
        "LABEL = data.LabelField()\n",
        "\n",
        "\n",
        "# We load the data and create the validation splits as before.\n",
        "\n",
        "# In[89]:\n",
        "\n",
        "\n",
        "from torchtext import datasets\n",
        "fields = {'text': ('text', TEXT), 'label': ('label', LABEL)}\n",
        "train_data, test_data = data.TabularDataset.splits(\n",
        "                                        path = data_path,\n",
        "                                        train = train_name,\n",
        "                                        test = validation_name,\n",
        "                                        format = 'tsv',\n",
        "                                        fields = fields,\n",
        "                                        skip_header = False\n",
        ")\n",
        "# train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
        "# valid_data = train_data\n",
        "train_data, valid_data = train_data.split(random_state = random.seed(SEED))\n",
        "\n",
        "\n",
        "# In[90]:\n",
        "\n",
        "\n",
        "print(vars(train_data[1]))\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'text': [152, 1306, 1403, 1139, 4153, 1108, 1176, 8506, 18959, 1179, 3263, 8468, 12786, 22081, 22615, 3177, 12786, 22081, 1186, 106, 106, 2508, 131, 1185, 2508, 5077, 1197, 24258, 1116, 4230, 2225, 3177, 144, 8734, 12487, 6718, 2430, 1160, 12261, 15861, 1158, 1339, 1339, 1114, 3632, 1104, 8730, 1339, 1114, 3632, 1104, 8730], 'label': '2'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMy50hu2HQKe",
        "colab_type": "code",
        "outputId": "f591238c-8f84-4163-eb26-0885ff0fe9e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "print(f\"Number of training examples: {len(train_data)}\")\n",
        "print(f\"Number of validation examples: {len(valid_data)}\")\n",
        "print(f\"Number of testing examples: {len(test_data)}\")\n",
        "\n",
        "\n",
        "# We can check an example and ensure that the text has already been numericalized.\n",
        "\n",
        "# In[92]:\n",
        "\n",
        "\n",
        "print(vars(train_data.examples[6]))\n",
        "\n",
        "\n",
        "# We can use the `convert_ids_to_tokens` to transform these indexes back into readable tokens.\n",
        "\n",
        "# In[93]:\n",
        "\n",
        "\n",
        "tokens = tokenizer.convert_ids_to_tokens(vars(train_data.examples[6])['text'])\n",
        "\n",
        "print(tokens)\n",
        "\n",
        "\n",
        "# Although we've handled the vocabulary for the text, we still need to build the vocabulary for the labels.\n",
        "\n",
        "# In[94]:\n",
        "\n",
        "\n",
        "LABEL.build_vocab(train_data)\n",
        "\n",
        "\n",
        "# In[95]:\n",
        "\n",
        "\n",
        "print(LABEL.vocab.stoi)\n",
        "\n",
        "\n",
        "# As before, we create the iterators. Ideally we want to use the largest batch size that we can as I've found this gives the best results for transformers.\n",
        "\n",
        "# In[96]:\n",
        "\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 7950\n",
            "Number of validation examples: 3407\n",
            "Number of testing examples: 2776\n",
            "{'text': [7085, 3080, 12890, 1161, 185, 2225, 5589, 12890, 7341, 4035, 8468, 10759, 194, 1143, 1325, 7363, 15027, 27466, 186, 6592, 10771, 2393, 2758, 1260, 1195, 22600, 1116, 1339, 1114, 3632, 1104, 8730], 'label': '2'}\n",
            "['Ma', '##mi', 'est', '##a', 'p', '##as', '##รก', 'est', '##oy', 'en', 'el', 'gym', 'y', 'me', 'll', '##ama', 'que', 'si', 'q', '##ui', '##ero', 'al', '##go', 'de', 'we', '##ndy', '##s', 'face', 'with', 'tears', 'of', 'joy']\n",
            "defaultdict(<function _default_unk_index at 0x7ffa4de53598>, {'2': 0, '1': 1, '0': 2})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HfcTlJmHWa3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    sort_key=lambda x: len(x.text), \n",
        "    batch_size = BATCH_SIZE, \n",
        "    device = device)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaX9fJ2BHcUs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "bert = BertModel.from_pretrained('bert-base-cased')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1zxSgTlHmkz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class BERTGRUSentiment(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_dim,\n",
        "                 output_dim,\n",
        "                 n_layers,\n",
        "                 bidirectional,\n",
        "                 dropout):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.bert = bert\n",
        "        \n",
        "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
        "        \n",
        "        self.rnn = nn.GRU(embedding_dim,\n",
        "                          hidden_dim,\n",
        "                          num_layers = n_layers,\n",
        "                          bidirectional = bidirectional,\n",
        "                          batch_first = True,\n",
        "                          dropout = 0 if n_layers < 2 else dropout)\n",
        "        # self.conv = nn.Conv1d(74,74, 3)\n",
        "        self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text):\n",
        "        \n",
        "        #text = [batch size, sent len]\n",
        "                \n",
        "        with torch.no_grad():\n",
        "            embedded = self.bert(text)[0]\n",
        "        #print(embedded.size())\n",
        "        #embedded = [batch size, sent len, emb dim]\n",
        "        \n",
        "        _, hidden = self.rnn(embedded)\n",
        "        #hidden = [n layers * n directions, batch size, emb dim]\n",
        "        #print(hidden.size())\n",
        "        if self.rnn.bidirectional:\n",
        "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "        else:\n",
        "            hidden = self.dropout(hidden[-1,:,:])\n",
        "        #print(hidden.size())\n",
        "        #hidden = [batch size, hid dim]\n",
        "        \n",
        "        output = self.out(hidden)\n",
        "        \n",
        "        #output = [batch size, out dim]\n",
        "        \n",
        "        return output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gO13qSpBH6Ey",
        "colab_type": "code",
        "outputId": "da8359b6-dbd0-4273-9a10-c9f118ac8303",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 3\n",
        "N_LAYERS = 3\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.25\n",
        "\n",
        "model = BERTGRUSentiment(bert,\n",
        "                         HIDDEN_DIM,\n",
        "                         OUTPUT_DIM,\n",
        "                         N_LAYERS,\n",
        "                         BIDIRECTIONAL,\n",
        "                         DROPOUT)\n",
        "\n",
        "\n",
        "# We can check how many parameters the model has. Our standard models have under 5M, but this one has 112M! Luckily, 110M of these parameters are from the transformer and we will not be training those.\n",
        "\n",
        "# In[100]:\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "\n",
        "\n",
        "# In order to freeze paramers (not train them) we need to set their `requires_grad` attribute to `False`. To do this, we simply loop through all of the `named_parameters` in our model and if they're a part of the `bert` transformer model, we set `requires_grad = False`. \n",
        "\n",
        "# In[101]:\n",
        "\n",
        "\n",
        "for name, param in model.named_parameters():                \n",
        "    if name.startswith('bert'):\n",
        "        param.requires_grad = False\n",
        "\n",
        "\n",
        "# We can now see that our model has under 3M trainable parameters, making it almost comparable to the `FastText` model. However, the text still has to propagate through the transformer which causes training to take considerably longer.\n",
        "\n",
        "# In[102]:\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 112,253,187 trainable parameters\n",
            "The model has 3,942,915 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoF5CJ3oX9jF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "94608d74-1590-47be-cc3e-31bc624503de"
      },
      "source": [
        "model"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BERTGRUSentiment(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (rnn): GRU(768, 256, num_layers=3, batch_first=True, dropout=0.25, bidirectional=True)\n",
              "  (out): Linear(in_features=512, out_features=3, bias=True)\n",
              "  (dropout): Dropout(p=0.25, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKjHj_RwH8iD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "451d29d9-dfda-43cf-a98e-be79e3ae4258"
      },
      "source": [
        "\n",
        "for name, param in model.named_parameters():                \n",
        "    if param.requires_grad:\n",
        "        print(name)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rnn.weight_ih_l0\n",
            "rnn.weight_hh_l0\n",
            "rnn.bias_ih_l0\n",
            "rnn.bias_hh_l0\n",
            "rnn.weight_ih_l0_reverse\n",
            "rnn.weight_hh_l0_reverse\n",
            "rnn.bias_ih_l0_reverse\n",
            "rnn.bias_hh_l0_reverse\n",
            "rnn.weight_ih_l1\n",
            "rnn.weight_hh_l1\n",
            "rnn.bias_ih_l1\n",
            "rnn.bias_hh_l1\n",
            "rnn.weight_ih_l1_reverse\n",
            "rnn.weight_hh_l1_reverse\n",
            "rnn.bias_ih_l1_reverse\n",
            "rnn.bias_hh_l1_reverse\n",
            "rnn.weight_ih_l2\n",
            "rnn.weight_hh_l2\n",
            "rnn.bias_ih_l2\n",
            "rnn.bias_hh_l2\n",
            "rnn.weight_ih_l2_reverse\n",
            "rnn.weight_hh_l2_reverse\n",
            "rnn.bias_ih_l2_reverse\n",
            "rnn.bias_hh_l2_reverse\n",
            "out.weight\n",
            "out.bias\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrmNft5kIBWK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "\n",
        "# In[105]:\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# Place the model and criterion onto the GPU (if available)\n",
        "\n",
        "# In[106]:\n",
        "\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "\n",
        "# Next, we'll define functions for: calculating accuracy, performing a training epoch, performing an evaluation epoch and calculating how long a training/evaluation epoch takes.\n",
        "\n",
        "# In[107]:\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sj5JrQYo31WA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def categorical_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
        "    correct = max_preds.squeeze(1).eq(y)\n",
        "    return correct.sum() / torch.FloatTensor([y.shape[0]])\n",
        "\n",
        "\n",
        "# In[108]:\n",
        "\n",
        "\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        predictions = model(batch.text).squeeze(1)\n",
        "        \n",
        "        loss = criterion(predictions, batch.label)\n",
        "        \n",
        "        acc = categorical_accuracy(predictions, batch.label)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYbK81uPIKE6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            predictions = model(batch.text).squeeze(1)\n",
        "            \n",
        "            loss = criterion(predictions, batch.label)\n",
        "            \n",
        "            acc = categorical_accuracy(predictions, batch.label)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "\n",
        "# In[110]:\n",
        "\n",
        "\n",
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jt2EwZE_IB57",
        "colab_type": "code",
        "outputId": "0a2ae93f-8adb-4df1-eda7-e85d10eb7fef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "N_EPOCHS = 20\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), model_save_name)\n",
        "        path = data_path+F\"{model_save_name}\"\n",
        "        torch.save(model.state_dict(), path)\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 54s\n",
            "\tTrain Loss: 1.050 | Train Acc: 46.43%\n",
            "\t Val. Loss: 1.049 |  Val. Acc: 44.86%\n",
            "Epoch: 02 | Epoch Time: 0m 52s\n",
            "\tTrain Loss: 1.000 | Train Acc: 48.95%\n",
            "\t Val. Loss: 1.014 |  Val. Acc: 46.88%\n",
            "Epoch: 03 | Epoch Time: 0m 55s\n",
            "\tTrain Loss: 0.993 | Train Acc: 49.65%\n",
            "\t Val. Loss: 1.023 |  Val. Acc: 44.43%\n",
            "Epoch: 04 | Epoch Time: 0m 52s\n",
            "\tTrain Loss: 0.976 | Train Acc: 49.85%\n",
            "\t Val. Loss: 0.968 |  Val. Acc: 50.22%\n",
            "Epoch: 05 | Epoch Time: 0m 50s\n",
            "\tTrain Loss: 0.955 | Train Acc: 52.36%\n",
            "\t Val. Loss: 0.959 |  Val. Acc: 51.48%\n",
            "Epoch: 06 | Epoch Time: 0m 52s\n",
            "\tTrain Loss: 0.922 | Train Acc: 54.10%\n",
            "\t Val. Loss: 0.968 |  Val. Acc: 52.72%\n",
            "Epoch: 07 | Epoch Time: 0m 52s\n",
            "\tTrain Loss: 0.894 | Train Acc: 56.20%\n",
            "\t Val. Loss: 0.994 |  Val. Acc: 52.47%\n",
            "Epoch: 08 | Epoch Time: 0m 53s\n",
            "\tTrain Loss: 0.870 | Train Acc: 58.06%\n",
            "\t Val. Loss: 0.990 |  Val. Acc: 48.67%\n",
            "Epoch: 09 | Epoch Time: 0m 49s\n",
            "\tTrain Loss: 0.818 | Train Acc: 60.66%\n",
            "\t Val. Loss: 1.031 |  Val. Acc: 48.95%\n",
            "Epoch: 10 | Epoch Time: 0m 53s\n",
            "\tTrain Loss: 0.803 | Train Acc: 62.77%\n",
            "\t Val. Loss: 1.023 |  Val. Acc: 48.16%\n",
            "Epoch: 11 | Epoch Time: 0m 53s\n",
            "\tTrain Loss: 0.773 | Train Acc: 65.01%\n",
            "\t Val. Loss: 1.044 |  Val. Acc: 49.39%\n",
            "Epoch: 12 | Epoch Time: 0m 53s\n",
            "\tTrain Loss: 0.716 | Train Acc: 68.31%\n",
            "\t Val. Loss: 1.109 |  Val. Acc: 50.05%\n",
            "Epoch: 13 | Epoch Time: 0m 50s\n",
            "\tTrain Loss: 0.645 | Train Acc: 71.29%\n",
            "\t Val. Loss: 1.118 |  Val. Acc: 51.28%\n",
            "Epoch: 14 | Epoch Time: 0m 51s\n",
            "\tTrain Loss: 0.615 | Train Acc: 73.41%\n",
            "\t Val. Loss: 1.227 |  Val. Acc: 49.56%\n",
            "Epoch: 15 | Epoch Time: 0m 55s\n",
            "\tTrain Loss: 0.571 | Train Acc: 75.97%\n",
            "\t Val. Loss: 1.279 |  Val. Acc: 49.32%\n",
            "Epoch: 16 | Epoch Time: 0m 53s\n",
            "\tTrain Loss: 0.513 | Train Acc: 78.64%\n",
            "\t Val. Loss: 1.353 |  Val. Acc: 48.71%\n",
            "Epoch: 17 | Epoch Time: 0m 53s\n",
            "\tTrain Loss: 0.474 | Train Acc: 80.04%\n",
            "\t Val. Loss: 1.440 |  Val. Acc: 48.25%\n",
            "Epoch: 18 | Epoch Time: 0m 52s\n",
            "\tTrain Loss: 0.402 | Train Acc: 83.88%\n",
            "\t Val. Loss: 1.493 |  Val. Acc: 47.79%\n",
            "Epoch: 19 | Epoch Time: 0m 52s\n",
            "\tTrain Loss: 0.379 | Train Acc: 84.63%\n",
            "\t Val. Loss: 1.653 |  Val. Acc: 49.79%\n",
            "Epoch: 20 | Epoch Time: 0m 51s\n",
            "\tTrain Loss: 0.331 | Train Acc: 86.99%\n",
            "\t Val. Loss: 1.707 |  Val. Acc: 49.24%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWLxw5OIp2HB",
        "colab_type": "code",
        "outputId": "15c9f6a8-9a81-4bc7-f38a-c04dc495ac6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "path = data_path+F\"{model_save_name}\"\n",
        "model.load_state_dict(torch.load(path))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEmZVcLs4Cws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def test_evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    epoch_all_acc = torch.FloatTensor([0,0,0,0,0,0])\n",
        "    print(epoch_all_acc)\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            predictions = model(batch.text).squeeze(1)\n",
        "            \n",
        "            loss = criterion(predictions, batch.label)\n",
        "            \n",
        "            acc,all_acc = test_categorical_accuracy(predictions, batch.label)\n",
        "            print(all_acc)\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "            epoch_all_acc += all_acc\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator),epoch_all_acc/len(iterator)\n",
        "\n",
        "def test_categorical_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "    count0,count1,count2 = torch.zeros(1),torch.zeros(1),torch.zeros(1)\n",
        "    total0,total1,total2 = torch.FloatTensor(1),torch.FloatTensor(1),torch.FloatTensor(1)\n",
        "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
        "    correct = max_preds.squeeze(1).eq(y)\n",
        "    \n",
        "    for j,i in enumerate(y.cpu().numpy()):\n",
        "      if i==0:\n",
        "        count0+=correct[j]\n",
        "        total0+=1\n",
        "      elif i==1:\n",
        "        count1+=correct[j]\n",
        "        total1+=1\n",
        "      elif i==2:\n",
        "        count2+=correct[j]\n",
        "      else:\n",
        "        print(i,i==0,i==1,i==2)\n",
        "        total2+=1\n",
        "    # print(count0,count1,count2,total0,total1,total2)\n",
        "    # print([count0/total0,count1/total1,count2/total2])\n",
        "    # print(torch.FloatTensor([count0/total0,count1/total1,count2/total2]))\n",
        "    # print(correct.sum() / torch.FloatTensor([y.shape[0]]))\n",
        "    # print(torch.FloatTensor([count0/total0,count1/total1,count2/total2]))\n",
        "    print(count0,count1,count2)\n",
        "    return correct.sum() / torch.FloatTensor([y.shape[0]]),torch.FloatTensor([count0/total0,count1/total1,count2/total2,count0,count1,count2])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IixrBAcbJeLJ",
        "colab_type": "code",
        "outputId": "6154042e-752d-4052-f8be-98f28a6414f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# model.load_state_dict(torch.load('tut6-model.pt'))\n",
        "\n",
        "test_loss, test_acc,test_all_acc = test_evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%',test_all_acc)\n",
        "\n",
        "\n",
        "# ## Inference\n",
        "# \n",
        "# We'll then use the model to test the sentiment of some sequences. We tokenize the input sequence, trim it down to the maximum length, add the special tokens to either side, convert it to a tensor, add a fake batch dimension and then pass it through our model.\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0., 0., 0., 0., 0., 0.])\n",
            "tensor([52.]) tensor([3.]) tensor([2.])\n",
            "tensor([ 9.4545e-01,  4.5455e-02, -4.6763e+26,  5.2000e+01,  3.0000e+00,\n",
            "         2.0000e+00])\n",
            "tensor([50.]) tensor([7.]) tensor([2.])\n",
            "tensor([ 8.7719e-01,  1.3725e-01, -4.6762e+26,  5.0000e+01,  7.0000e+00,\n",
            "         2.0000e+00])\n",
            "tensor([49.]) tensor([5.]) tensor([5.])\n",
            "tensor([ 8.0328e-01,  1.1111e-01, -1.1690e+27,  4.9000e+01,  5.0000e+00,\n",
            "         5.0000e+00])\n",
            "tensor([43.]) tensor([6.]) tensor([2.])\n",
            "tensor([ 7.8182e-01,  1.1765e-01, -4.6766e+26,  4.3000e+01,  6.0000e+00,\n",
            "         2.0000e+00])\n",
            "tensor([48.]) tensor([13.]) tensor([2.])\n",
            "tensor([ 8.1356e-01,  2.7083e-01, -4.6766e+26,  4.8000e+01,  1.3000e+01,\n",
            "         2.0000e+00])\n",
            "tensor([46.]) tensor([12.]) tensor([4.])\n",
            "tensor([ 7.9310e-01,  2.3529e-01, -9.3528e+26,  4.6000e+01,  1.2000e+01,\n",
            "         4.0000e+00])\n",
            "tensor([35.]) tensor([13.]) tensor([6.])\n",
            "tensor([ 6.7308e-01,  2.4074e-01, -1.4029e+27,  3.5000e+01,  1.3000e+01,\n",
            "         6.0000e+00])\n",
            "tensor([39.]) tensor([10.]) tensor([4.])\n",
            "tensor([ 7.8000e-01,  1.6949e-01, -9.3529e+26,  3.9000e+01,  1.0000e+01,\n",
            "         4.0000e+00])\n",
            "tensor([55.]) tensor([9.]) tensor([0.])\n",
            "tensor([ 0.7534,  0.2308, -0.0000, 55.0000,  9.0000,  0.0000])\n",
            "tensor([42.]) tensor([11.]) tensor([6.])\n",
            "tensor([ 7.5000e-01,  2.5581e-01, -1.4029e+27,  4.2000e+01,  1.1000e+01,\n",
            "         6.0000e+00])\n",
            "tensor([52.]) tensor([13.]) tensor([3.])\n",
            "tensor([ 8.0000e-01,  3.0233e-01, -7.0145e+26,  5.2000e+01,  1.3000e+01,\n",
            "         3.0000e+00])\n",
            "tensor([50.]) tensor([8.]) tensor([7.])\n",
            "tensor([ 7.2464e-01,  2.4242e-01, -1.6367e+27,  5.0000e+01,  8.0000e+00,\n",
            "         7.0000e+00])\n",
            "tensor([44.]) tensor([6.]) tensor([6.])\n",
            "tensor([ 7.5862e-01,  1.4286e-01, -1.4029e+27,  4.4000e+01,  6.0000e+00,\n",
            "         6.0000e+00])\n",
            "tensor([60.]) tensor([6.]) tensor([6.])\n",
            "tensor([ 8.4507e-01,  1.6667e-01, -1.4029e+27,  6.0000e+01,  6.0000e+00,\n",
            "         6.0000e+00])\n",
            "tensor([52.]) tensor([6.]) tensor([4.])\n",
            "tensor([ 8.2540e-01,  1.3953e-01, -9.3528e+26,  5.2000e+01,  6.0000e+00,\n",
            "         4.0000e+00])\n",
            "tensor([59.]) tensor([5.]) tensor([4.])\n",
            "tensor([ 7.6623e-01,  1.6129e-01, -9.3529e+26,  5.9000e+01,  5.0000e+00,\n",
            "         4.0000e+00])\n",
            "tensor([54.]) tensor([4.]) tensor([2.])\n",
            "tensor([ 7.8261e-01,  1.0811e-01, -4.6766e+26,  5.4000e+01,  4.0000e+00,\n",
            "         2.0000e+00])\n",
            "tensor([57.]) tensor([3.]) tensor([5.])\n",
            "tensor([ 7.5000e-01,  9.3750e-02, -1.1691e+27,  5.7000e+01,  3.0000e+00,\n",
            "         5.0000e+00])\n",
            "tensor([69.]) tensor([3.]) tensor([3.])\n",
            "tensor([ 8.7342e-01,  1.0345e-01, -7.0143e+26,  6.9000e+01,  3.0000e+00,\n",
            "         3.0000e+00])\n",
            "tensor([53.]) tensor([7.]) tensor([6.])\n",
            "tensor([ 7.9104e-01,  2.1212e-01, -1.4029e+27,  5.3000e+01,  7.0000e+00,\n",
            "         6.0000e+00])\n",
            "tensor([57.]) tensor([6.]) tensor([8.])\n",
            "tensor([ 8.1429e-01,  1.9355e-01, -1.8706e+27,  5.7000e+01,  6.0000e+00,\n",
            "         8.0000e+00])\n",
            "tensor([50.]) tensor([0.]) tensor([2.])\n",
            "tensor([ 9.8039e-01,  0.0000e+00, -4.6765e+26,  5.0000e+01,  0.0000e+00,\n",
            "         2.0000e+00])\n",
            "Test Loss: 0.974 | Test Acc: 49.17% tensor([ 8.0376e-01,  1.6729e-01, -9.4590e+26,  5.0727e+01,  7.0909e+00,\n",
            "         4.0455e+00])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONmKOMZmJtfk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8f875047-7a43-4d85-d7ca-bfd7d55e5720"
      },
      "source": [
        "def predict_sentiment(model, tokenizer, sentence):\n",
        "    model.eval()\n",
        "    tokens = tokenizer.tokenize(sentence)\n",
        "    tokens = tokens[:max_input_length-2]\n",
        "    indexed = [init_token_idx] + tokenizer.convert_tokens_to_ids(tokens) + [eos_token_idx]\n",
        "    tensor = torch.LongTensor(indexed).to(device)\n",
        "    tensor = tensor.unsqueeze(0)\n",
        "    prediction = torch.sigmoid(model(tensor))\n",
        "    # ind = np.argmax(np.array(prediction))\n",
        "    # if ind ==0:\n",
        "    #   print('neutral')\n",
        "    # elif ind == 1:\n",
        "    #   print(\"positive\")\n",
        "    # else:\n",
        "    #   print(\"negative\")\n",
        "    print(prediction)\n",
        "predict_sentiment(model, tokenizer, \"This film is terrible\")\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "predict_sentiment(model, tokenizer, \"This film is great\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.4250, 0.3900, 0.6735]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "tensor([[0.7224, 0.5535, 0.2736]], device='cuda:0', grad_fn=<SigmoidBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SO-V3S1HV2Oi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "while True:\n",
        "  sent = input('->')\n",
        "  if sent != '$':\n",
        "    predict_sentiment(model, tokenizer, sent)\n",
        "  else:\n",
        "    break\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}